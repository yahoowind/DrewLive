import requests
from collections import defaultdict
import re
import time
from datetime import datetime

playlist_urls = [
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/DaddyLive.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/DrewAll.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/JapanTV.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/PlexTV.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/PlutoTV.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/TubiTV.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/DrewLiveVOD.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/UDPTV.m3u",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/TVPass.m3u",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/Radio.m3u8",
    "http://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/DaddyLiveEvents.m3u8",
    "http://drewlive24.duckdns.org:8081/PPVLand.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/StreamEast.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/FSTV24.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/TheTVApp.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/Roku.m3u8",
    "http://drewlive24.duckdns.org:8081/Tims247.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/LGTV.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/AriaPlus.m3u8",
    "http://drewlive24.duckdns.org:8081/Zuzz.m3u8",
    "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/SamsungTVPlus.m3u8"
]

UDPTV_URL = "https://raw.githubusercontent.com/Drewski2423/DrewLive/refs/heads/main/UDPTV.m3u"
EPG_URL = "http://drewlive24.duckdns.org:8081/merged2_epg.xml.gz"
OUTPUT_FILE = "MergedCleanPlaylist.m3u8"
REMOVED_FILE = "Removed_NSFW.m3u8"

# Only allow real streaming protocols
VALID_URL_PATTERN = re.compile(r'^(https?|rtmp|rtsp|udp)://', re.IGNORECASE)

def fetch_playlist(url, retries=3, delay=5):
    """Fetch playlist with retry, clean junk lines like '*' and invalid content."""
    print(f"üì° Fetching: {url}")
    for attempt in range(1, retries + 1):
        try:
            res = requests.get(url, timeout=15)
            res.raise_for_status()
            content = res.content.decode('utf-8', errors='ignore')
            if "#EXTM3U" not in content:
                print(f"‚ö†Ô∏è Skipping invalid playlist (no #EXTM3U) from {url}")
                return []
            # Clean junk lines: drop '*', blanks, proxy error messages
            return [
                line for line in content.splitlines()
                if line.strip() 
                and line.strip() != "*"
                and not line.lower().startswith("proxy failed")
            ]
        except Exception as e:
            print(f"‚ùå Attempt {attempt} failed for {url}: {e}")
            if "raw.githubusercontent.com" not in url and attempt < retries:
                print(f"‚è≥ Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                break
    print(f"‚ùå Failed to fetch {url} after {retries} attempts.")
    return []

def extract_timestamp_from_udptv(lines):
    for line in lines:
        if line.strip().startswith("# Last forced update:"):
            print(f"‚úÖ Found UDPTV timestamp: {line.strip()}")
            return line.strip()
    print("‚ö†Ô∏è No update timestamp found in UDPTV.")
    return None

def parse_playlist(lines, source_url="Unknown"):
    parsed = []
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        if line.startswith("#EXTINF:"):
            extinf = line
            headers = []
            i += 1
            while i < len(lines) and lines[i].strip().startswith("#") and not lines[i].strip().startswith("#EXTINF:"):
                headers.append(lines[i].strip())
                i += 1
            # Validate stream URL
            if i < len(lines) and lines[i].strip() and not lines[i].strip().startswith("#"):
                url = lines[i].strip()
                if VALID_URL_PATTERN.match(url):
                    parsed.append((extinf, tuple(headers), url))
                else:
                    print(f"‚ö†Ô∏è Skipped invalid URL in {source_url}: {url}")
                i += 1
            else:
                print(f"‚ö†Ô∏è Skipped orphaned EXTINF in {source_url}")
        else:
            i += 1
    print(f"‚úÖ Parsed {len(parsed)} channels from {source_url}")
    return parsed

def is_nsfw(extinf, headers, url):
    nsfw_keywords = ['nsfw', 'xxx', 'porn']
    extinf_lower = extinf.lower()
    headers_lower = ' '.join(headers).lower()
    url_lower = url.lower()
    group_match = re.search(r'group-title="([^"]+)"', extinf_lower)
    if group_match and any(k in group_match.group(1) for k in nsfw_keywords):
        return True
    combined_text = f"{extinf_lower} {headers_lower} {url_lower}"
    return any(k in combined_text for k in nsfw_keywords)

def write_merged_playlist(all_channels, timestamp_line):
    lines = [f'#EXTM3U url-tvg="{EPG_URL}"']
    if timestamp_line:
        lines.append(timestamp_line)
    lines.append("")

    sortable = []
    for extinf, headers, url in all_channels:
        group_match = re.search(r'group-title="([^"]+)"', extinf)
        group = group_match.group(1) if group_match else "Other"
        title_match = re.search(r',([^,]+)$', extinf)
        title = title_match.group(1).strip() if title_match else ""
        sortable.append((group.lower(), title.lower(), group, extinf, headers, url))

    sorted_channels = sorted(sortable)

    current_group = None
    count = 0

    for _, _, group_name, extinf, headers, url in sorted_channels:
        if group_name != current_group:
            if current_group is not None:
                lines.append("")
            lines.append(f"#EXTGRP:{group_name}")
            current_group = group_name
        lines.append(extinf)
        lines.extend(headers)
        lines.append(url)
        count += 1

    if lines and lines[-1] == "":
        lines.pop()

    final_output = '\n'.join(lines) + '\n'

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(final_output)

    print(f"\n‚úÖ Wrote {count} clean channels to {OUTPUT_FILE} ({len(final_output.splitlines())} lines).")

def write_removed_channels(nsfw_channels):
    if not nsfw_channels:
        return
    with open(REMOVED_FILE, 'w', encoding='utf-8') as f:
        f.write("#EXTM3U\n")
        for extinf, headers, url in nsfw_channels:
            f.write(extinf + "\n")
            for h in headers:
                f.write(h + "\n")
            f.write(url + "\n\n")
    print(f"üóëÔ∏è Logged {len(nsfw_channels)} removed NSFW/XXX/Porn entries to {REMOVED_FILE}")

if __name__ == "__main__":
    print(f"üöÄ Starting merge: {datetime.now()}\n")

    all_channels = []
    timestamp_line = None

    # Process UDPTV first
    print(f"--- Processing UDPTV: {UDPTV_URL} ---")
    udptv_lines = fetch_playlist(UDPTV_URL)
    timestamp_line = extract_timestamp_from_udptv(udptv_lines)
    all_channels.extend(parse_playlist(udptv_lines, UDPTV_URL))

    # Process remaining playlists
    print(f"\n--- Processing other playlists ---")
    for url in playlist_urls:
        if url == UDPTV_URL:
            continue
        lines = fetch_playlist(url)
        all_channels.extend(parse_playlist(lines, url))

    # Filter NSFW content
    nsfw_channels = [entry for entry in all_channels if is_nsfw(*entry)]
    clean_channels = [entry for entry in all_channels if not is_nsfw(*entry)]

    write_removed_channels(nsfw_channels)
    write_merged_playlist(clean_channels, timestamp_line)

    print(f"\n‚úÖ Merge complete: {datetime.now()}")
